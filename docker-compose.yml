version: '3'

services:
  server:
    container_name: executor-component
    image: executor-component:latest
    build: 
      context: .
      dockerfile: Dockerfile
    ports:
      - ${PORT:-8081}:${PORT:-8081}
    env_file:
      - .env
    networks:
      - executor-component-network
  llama3-8b:
    container_name: llama3-8b
    image: llama3-8b:latest
    build: 
      context: ./Ollama
      dockerfile: Dockerfile.predefined-model
      args:
        MODEL_NAME: "llama3"
    ports:
      - "11435:11434"
    volumes:
      - llama3-8b_data:/root/.ollama
    networks:
      - executor-component-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  llama3-8b_data:

networks:
  executor-component-network:
    driver: bridge